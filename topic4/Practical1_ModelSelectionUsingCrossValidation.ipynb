{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of contents</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#practical_plan\">Practical and Data Overview</a></li>\n",
    "        <li><a href=\"#reading_data\">Preparation: importing packages and loading data </a></li>\n",
    "        <li><a href=\"#naive_rf\">A Naive RF Model</a></li>\n",
    "        <li><a href=\"#good_rf\">A Tuned RF Model  </a></li>\n",
    "        <li><a href=\"#task\">Your Task: Implement a Naive and a Tuned KNN Classifier</a></li>\n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"practical_plan\">Practical and Data Overview</h2>\n",
    "\n",
    "- This practical will examine the effect of hyperparameter tuning on model performance. \n",
    "- Models: we will train and evaluate two classification models: Random Forest and K-nearest Neighbours.\n",
    "- Data: \n",
    "    - We will be using a well-known diabetes dataset, which is available from many ML reposiroties, e.g. UCI\n",
    "        - UCI link: https://archive.ics.uci.edu/ml/support/diabetes\n",
    "        - I personally used the github link provided by Jason Brown-Lee (Machine Learning Mastery's blogger):         \n",
    "            - The Dataset: https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
    "            - Dataset Description: https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"reading_data\">Importing packages and reading data</h2>\n",
    "\n",
    "- Data Download Instructions:             \n",
    "    - Download the dataset and place it in your local working directory, the same location as your python file.\n",
    "    - Save it with the filename: pima-indians-diabetes.csv\n",
    "\n",
    "<b>Note: </b> see how the data is saved with data only: column names are provided in a separate file (for you to understand what each data column means). This is done to ease the process of loading the data directly into a numpy array (2D matrix). We can now load the file as a matrix of numbers using the NumPy function loadtxt(), which is available from the numpy library. Hence, the following imports are needed: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: i'm only importing pprint because I'd like to be able to use more 'deocrative' printing options\n",
    "## for the decimal points\n",
    "from pprint import pprint \n",
    "from numpy import loadtxt\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## the hyperparameters will be selected through a cross-validation experiment, hence we need the following packages:\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score,  train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the dataset: \n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The .names file shows that the dataset contains the following 9 columns: \n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "\n",
    "- Column 9 is the class label. \n",
    "- Since the .csv file has no labels, and we did not load it into a dataframe, we have to treat it as an array. \n",
    "- Arrays (and matrices) are referenced by indices. \n",
    "- dataset[:, 8] returns all rows (designated by :) and the 9th column (please note that python indices start at 0).\n",
    "- dataset[:, 0:7] returns all rows for columns 1-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(768, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = dataset[:,0:7]\n",
    "y = dataset[:,8]\n",
    "\n",
    "## check the types: both are numpy  arrays\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"naive_rf\">A Naive RF Model</h2>\n",
    "\n",
    "- We will first implement a 'naive' RF classifier using the default parameters and evaluate the model using the regular hold-out method we have been using so far. Steps: \n",
    "\n",
    "    1. Split our data into training and testing samples.\n",
    "    2. Initialise a RF classifier using all default parameters\n",
    "    3. Train the RF using the .fit function and the training data+labels\n",
    "    4. Extract the classifier's predictions on test data (X_test) using the .predict function\n",
    "    5. Examine the classifier's performance on unseen data (y_test) by comparing with the classifier's predictions (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.82      0.78       103\n",
      "         1.0       0.54      0.43      0.48        51\n",
      "\n",
      "    accuracy                           0.69       154\n",
      "   macro avg       0.64      0.62      0.63       154\n",
      "weighted avg       0.67      0.69      0.68       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "naive_classifier = RandomForestClassifier()\n",
    "naive_classifier.fit(X_train, y_train)  \n",
    "y_pred= naive_classifier.predict(X_test)  \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I ran the model a few times and got an accuracy score ranging from 79% to 81%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"good_rf\">A Tuned RF Model</h2>\n",
    "\n",
    "- Now letâ€™s tune our hyperparameters using cross-validation.\n",
    "- Before that, let's examine what the parameters used in the classification were: \n",
    "    - You can find out what each parameter means by reading the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': None,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "pprint(naive_classifier.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The process for finding the right hyperparameters is still somewhat of a dark art, and it can be perfomred using a three-way holdout (as seen in the lecture) or using cross-validation. In this practical, will use cross-validation to find the optimal hyperparameters. The data used in the hyperparameter-tuning phase is X_train (and y_train), because we will be evaluating the best algorithm's performance on X_test (and y_test) in order to obtain a 'final' estimate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the 'parameter grid'\n",
    " - Set up possible values of parameters to optimize over\n",
    " - The 'norm' is to use a dictionary object (see 5.5 of https://docs.python.org/3/tutorial/datastructures.html) to store 'a parameter grid'\n",
    " - A parameter grid contains all possible values of the hyperparameters we would like to 'tune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "            \"min_samples_leaf\": [10, 20, 30],\n",
    "            'n_estimators': [20, 60, 100],\n",
    "             'max_features' : [3,5,8]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using cross validation to tune a RF classifier for a given hyperparameter grid. \n",
    "\n",
    "- A grid search across Cartesian products of sets of hyperparameters. What is meant by the cartesian product is the creation of 'a set of parameters' for every possible combination of the min_samples_split, n_estimators and max_features listed in the hyperparameter grid we've created. e.g. \n",
    " \n",
    "        - min_samples_leaf = 1, n_estimators = 10, max_features = 3\n",
    "        - min_samples_leaf = 1, n_estimators = 10, max_features = 5\n",
    "        - min_samples_leaf = 1, n_estimators = 100, max_features = 3\n",
    "        - min_samples_leaf = 1, n_estimators = 100, max_features = 5\n",
    "        -etc...\n",
    "\n",
    "- Note: the code will take a while to return an output, since we are fitting the classifier over all possible combinations of the parameters (this is what a grid search is). So, we are running the random forest using all permuatations listed above.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a gridsearch object with the random forest classifier and the parameter candidates obtained from parameter_grid\n",
    "- We are using a 7-fold cross validation (no scientific reason behind it - best go with 10, but this will be more computationally expensive)\n",
    "- Documentation of GridSearchCV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_grid = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameter_grid, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The best parameters found are: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "63 fits failed out of a total of 189.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "63 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
      "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
      "ValueError: max_features must be in (0, n_features]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/changhyun/King's College London/ml_bioinfo_hi/mlBioHealth/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.77526496 0.76869682 0.77849306 0.77035752 0.78013509 0.76871548\n",
      " 0.77198089 0.77037618 0.77037618 0.77194357 0.77198089 0.78013509\n",
      " 0.76384535 0.77199955 0.77198089 0.76712942 0.77205553 0.77035752\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_features': 3, 'min_samples_leaf': 20, 'n_estimators': 60}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the cross validated grid search on the data \n",
    "classifier_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\" The best parameters found are: \")\n",
    "classifier_grid.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have found the best parameters, we can use those to evaluate the best model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = classifier_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating the best model's performance\n",
    "- Using the test dataset, we evaluate the performance of the model created using the best hyperparameters (best_model) using X_test (and its corresponding labels y_test). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##accuracy_over_runs = cross_val_score(best_model, X_test, y_test, cv=3)\n",
    "y_pred= best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.84      0.80       103\n",
      "         1.0       0.59      0.45      0.51        51\n",
      "\n",
      "    accuracy                           0.71       154\n",
      "   macro avg       0.67      0.65      0.65       154\n",
      "weighted avg       0.70      0.71      0.70       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(accuracy_over_runs)\n",
    "#accuracy_over_runs.mean()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depending on the run, we've achieved an unspectacular improvement in accuracy of 1-8%.\n",
    "- But remember: \n",
    "   - Depending on the application, this could be a significant benefit :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"task\">Implement a KNN Version of the Naive and Refined Models</h2>\n",
    "\n",
    "- Use cross-validation to tune the hyperparameter of a KNN classifier on the same X,y data.  \n",
    "- Guide:\n",
    "    - You can view KNN's parameters using the function: my_knn_model.get_params()\n",
    "    - You can also lookup KNNs in the scikit learn API documentation. \n",
    "        - From our k-nearest neighbour lecture, we know that K is the most important parameter to specify. \n",
    "        - Try a large variety of values for K .\n",
    "    - The needed library has already been imported for you (from sklearn.neighbors import KNeighborsClassifier\n",
    "), so you can jumpt right into building/tuning/training/testing the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.90      0.81       103\n",
      "         1.0       0.63      0.33      0.44        51\n",
      "\n",
      "    accuracy                           0.71       154\n",
      "   macro avg       0.68      0.62      0.62       154\n",
      "weighted avg       0.70      0.71      0.69       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Your Solution Here ###\n",
    "param_grid = {\n",
    "    'n_neighbors':[1,2,3,5,10,20,25,50, 100, 200, 500]\n",
    "}\n",
    "\n",
    "validated_classifier = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid, cv=7)\n",
    "\n",
    "validated_classifier.fit(X_train, y_train)\n",
    "print(validated_classifier.best_params_)\n",
    "\n",
    "best_knn = validated_classifier.best_estimator_\n",
    "\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlBioHealth",
   "language": "python",
   "name": "mlbiohealth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
